time=2025-10-19T16:04:20.138+08:00 level=INFO source=routes.go:1511 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Users\\User\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-10-19T16:04:20.183+08:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-10-19T16:04:20.184+08:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-10-19T16:04:20.187+08:00 level=INFO source=routes.go:1564 msg="Listening on 127.0.0.1:11434 (version 0.12.6)"
time=2025-10-19T16:04:20.187+08:00 level=INFO source=runner.go:80 msg="discovering available GPUs..."
time=2025-10-19T16:04:24.572+08:00 level=INFO source=types.go:112 msg="inference compute" id=GPU-c627a3ba-5767-b83b-d856-ef9ff780a421 library=CUDA compute=12.0 name=CUDA0 description="NVIDIA GeForce RTX 5070" libdirs=ollama,cuda_v12 driver=12.9 pci_id=01:00.0 type=discrete total="11.9 GiB" available="11.2 GiB"
time=2025-10-19T16:04:24.572+08:00 level=INFO source=routes.go:1605 msg="entering low vram mode" "total vram"="11.9 GiB" threshold="20.0 GiB"
[GIN] 2025/10/19 - 16:05:59 | 200 |     89.2974ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 16:05:59 | 200 |     27.0757ms |       127.0.0.1 | POST     "/api/show"
time=2025-10-19T16:06:02.764+08:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-19T16:06:02.764+08:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-19T16:06:02.764+08:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=14 efficiency=8 threads=20
time=2025-10-19T16:06:02.765+08:00 level=INFO source=server.go:216 msg="enabling flash attention"
time=2025-10-19T16:06:02.766+08:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\User\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --model C:\\Users\\User\\.ollama\\models\\blobs\\sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --port 59681"
time=2025-10-19T16:06:02.769+08:00 level=INFO source=server.go:676 msg="loading model" "model layers"=37 requested=-1
time=2025-10-19T16:06:02.769+08:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-19T16:06:02.769+08:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-19T16:06:02.769+08:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=14 efficiency=8 threads=20
time=2025-10-19T16:06:02.769+08:00 level=INFO source=server.go:682 msg="system memory" total="31.8 GiB" free="13.2 GiB" free_swap="13.1 GiB"
time=2025-10-19T16:06:02.769+08:00 level=INFO source=server.go:690 msg="gpu memory" id=GPU-c627a3ba-5767-b83b-d856-ef9ff780a421 library=CUDA available="10.4 GiB" free="10.8 GiB" minimum="457.0 MiB" overhead="0 B"
time=2025-10-19T16:06:02.793+08:00 level=INFO source=runner.go:1332 msg="starting ollama engine"
time=2025-10-19T16:06:02.806+08:00 level=INFO source=runner.go:1367 msg="Server listening on 127.0.0.1:59681"
time=2025-10-19T16:06:02.813+08:00 level=INFO source=runner.go:1205 msg=load request="{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:6 GPULayers:37[ID:GPU-c627a3ba-5767-b83b-d856-ef9ff780a421 Layers:37(0..36)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-10-19T16:06:02.827+08:00 level=INFO source=ggml.go:134 msg="" architecture=qwen3 file_type=Q4_K_M name="Qwen3 8B" description="" num_tensors=399 num_key_values=29
load_backend: loaded CPU backend from C:\Users\User\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 5070, compute capability 12.0, VMM: yes, ID: GPU-c627a3ba-5767-b83b-d856-ef9ff780a421
load_backend: loaded CUDA backend from C:\Users\User\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-10-19T16:06:02.907+08:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-10-19T16:06:03.157+08:00 level=INFO source=runner.go:1205 msg=load request="{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:6 GPULayers:37[ID:GPU-c627a3ba-5767-b83b-d856-ef9ff780a421 Layers:37(0..36)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-10-19T16:06:03.238+08:00 level=INFO source=runner.go:1205 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:6 GPULayers:37[ID:GPU-c627a3ba-5767-b83b-d856-ef9ff780a421 Layers:37(0..36)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-10-19T16:06:03.238+08:00 level=INFO source=device.go:206 msg="model weights" device=CUDA0 size="4.5 GiB"
time=2025-10-19T16:06:03.238+08:00 level=INFO source=device.go:211 msg="model weights" device=CPU size="333.8 MiB"
time=2025-10-19T16:06:03.238+08:00 level=INFO source=device.go:217 msg="kv cache" device=CUDA0 size="576.0 MiB"
time=2025-10-19T16:06:03.238+08:00 level=INFO source=device.go:228 msg="compute graph" device=CUDA0 size="126.0 MiB"
time=2025-10-19T16:06:03.238+08:00 level=INFO source=device.go:233 msg="compute graph" device=CPU size="8.0 MiB"
time=2025-10-19T16:06:03.238+08:00 level=INFO source=device.go:238 msg="total memory" size="5.6 GiB"
time=2025-10-19T16:06:03.238+08:00 level=INFO source=sched.go:482 msg="loaded runners" count=1
time=2025-10-19T16:06:03.238+08:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-19T16:06:03.239+08:00 level=INFO source=server.go:1306 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T16:06:03.238+08:00 level=INFO source=ggml.go:480 msg="offloading 36 repeating layers to GPU"
time=2025-10-19T16:06:03.239+08:00 level=INFO source=ggml.go:487 msg="offloading output layer to GPU"
time=2025-10-19T16:06:03.239+08:00 level=INFO source=ggml.go:492 msg="offloaded 37/37 layers to GPU"
time=2025-10-19T16:06:04.991+08:00 level=INFO source=server.go:1310 msg="llama runner started in 2.23 seconds"
[GIN] 2025/10/19 - 16:06:06 | 200 |    6.5968171s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 16:06:08 | 200 |     50.6134ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 16:06:08 | 200 |     47.9968ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 16:06:08 | 200 |     50.0913ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 16:06:08 | 200 |     48.3932ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 16:06:08 | 200 |     47.0726ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 16:06:09 | 200 |     45.0097ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 16:06:09 | 200 |     46.9158ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 16:06:09 | 200 |    1.1965888s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 16:06:10 | 200 |     45.7196ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 16:06:10 | 200 |     47.1763ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 16:06:10 | 200 |     44.0693ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 16:06:11 | 200 |    2.7201086s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 16:06:11 | 200 |     48.4327ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 16:06:11 | 200 |     44.6487ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 16:06:11 | 200 |     46.1213ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 16:07:37 | 200 |     25.2895ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 16:07:37 | 200 |     27.3463ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 16:07:38 | 200 |    1.3880001s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 16:07:38 | 200 |     25.6419ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 16:07:38 | 200 |     24.2636ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 16:07:38 | 200 |     25.7653ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 16:09:16 | 200 |     55.3799ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 16:09:16 | 200 |     49.5661ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 16:09:18 | 200 |    1.8741969s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 16:09:18 | 200 |     45.3728ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 16:09:18 | 200 |     48.5298ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 16:09:18 | 200 |     44.9527ms |       127.0.0.1 | POST     "/api/show"
ggml_backend_cuda_device_get_memory utilizing NVML memory reporting free: 5632344064 total: 12820938752
